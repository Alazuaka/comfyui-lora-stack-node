# üìÇ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –º–∞—Ä—à—Ä—É—Ç–æ–≤ `/alazuka/‚Ä¶`

# | Endpoint | –ú–µ—Ç–æ–¥ | –ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ |
# |----------|-------|------------|
# | `/alazuka/file/{type}/{filename}` | `GET` | –ü–æ–ª—É—á–∏—Ç—å –ª—é–±–æ–π —Ñ–∞–π–ª (json, image, model, etc) |
# | `/alazuka/related/{type}/{basename}` | `GET` | –ü–æ–ª—É—á–∏—Ç—å —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã (preview, json –∏ –¥—Ä.) |
# | `/alazuka/savefile/{type}/{target}` | `POST` | –°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ñ–∞–π–ª –∫–∞–∫ –ø—Ä–µ–≤—å—é –∏–ª–∏ –¥—Ä—É–≥–æ–π —Ç–∏–ø |

import os
import re
import json
import folder_paths

from PIL import Image, ExifTags
from aiohttp import web
from server import PromptServer
from folder_paths import get_full_path

# –ü—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –º–æ–∂–Ω–æ –≤—ã–∑–≤–∞—Ç—å register_endpoints –∑–¥–µ—Å—å
# –∏–ª–∏ –æ—Å—Ç–∞–≤–∏—Ç—å –∫–∞–∫ –µ—Å—Ç—å, –µ—Å–ª–∏ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç —á–µ—Ä–µ–∑ __init__.py



def extract_metadata_from_media(filepath):
    ext = os.path.splitext(filepath)[1].lower()
    if ext in ['.jpg', '.jpeg', '.png']:
        return extract_universal_metadata(filepath)
    else:
        return {"error": f"Unsupported file type: {ext}"}

def extract_universal_metadata(filepath):
    metadata = {
        "path": filepath,
        "size": None,
        "format": "JPEG" if filepath.lower().endswith(('.jpg', '.jpeg')) else "PNG",
        "prompt": "",
        "negative_prompt": "",
        "workflow": "",
        "parameters": {},
        "is_NSFW": False
    }

    try:
        with Image.open(filepath) as img:
            metadata["size"] = img.size
            
            # –û–±—â–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è –≤—Å–µ—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤
            text_data = {}
            if hasattr(img, '_getexif'):
                exif_data = img._getexif() or {}
                for tag, value in exif_data.items():
                    tag_name = ExifTags.TAGS.get(tag, tag)
                    if tag == 37510:  # UserComment
                        try:
                            if isinstance(value, bytes):
                                value = value.decode('utf-16-be' if value.startswith(b'UNICODE\x00\x00') else 'utf-8', errors='ignore')
                            text_data["UserComment"] = value
                        except Exception as e:
                            text_data["UserComment"] = f"Decode error: {str(e)}"
            
            # –î–ª—è PNG –∏ –¥—Ä—É–≥–∏—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤
            if hasattr(img, 'info'):
                for key, value in img.info.items():
                    if isinstance(value, str):
                        text_data[key] = value
            
            # –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
            search_text = "\n".join([str(v) for v in text_data.values()])
            parse_any_metadata(search_text, metadata)
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ NSFW
            metadata["is_NSFW"] = check_nsfw(metadata["prompt"])
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å–µ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
            metadata["text_data"] = text_data
            
    except Exception as e:
        metadata["error"] = str(e)
    
    return metadata

def parse_any_metadata(text, metadata):
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–µ—Ä –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤ –ª—é–±–æ–º –º–µ—Å—Ç–µ —Ç–µ–∫—Å—Ç–∞"""
    if not text:
        return
    
    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞
    text = text.replace('\r\n', '\n').replace('\r', '\n')
    
    # –ü–æ–∏—Å–∫ –Ω–µ–≥–∞—Ç–∏–≤–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞ (—Ä–∞–∑–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –Ω–∞–ø–∏—Å–∞–Ω–∏—è)
    neg_patterns = [
        r"negative prompt:\s*(.*?)(?=\n\w+:|$)",
        r"neg prompt:\s*(.*?)(?=\n\w+:|$)",
        r"negative_prompt:\s*(.*?)(?=\n\w+:|$)"
    ]
    
    for pattern in neg_patterns:
        if match := re.search(pattern, text, re.IGNORECASE | re.DOTALL):
            metadata["negative_prompt"] = match.group(1).strip()
            break
    
    # –ü–æ–∏—Å–∫ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞ (–±–µ—Ä–µ–º —Ç–µ–∫—Å—Ç –¥–æ –Ω–µ–≥–∞—Ç–∏–≤–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞ –∏–ª–∏ –≤–µ—Å—å —Ç–µ–∫—Å—Ç)
    if metadata["negative_prompt"]:
        split_pos = text.lower().find("negative prompt")
        metadata["prompt"] = text[:split_pos].strip()
    else:
        metadata["prompt"] = text.strip()
    
    # –ü–æ–∏—Å–∫ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
    param_patterns = {
        "Steps": r"Steps:\s*(\d+)",
        "Sampler": r"Sampler:\s*([^\n]+)",
        "CFG scale": r"CFG scale:\s*([\d.]+)",
        "Seed": r"Seed:\s*(\d+)",
        "Size": r"Size:\s*(\d+x\d+)",
        "Model": r"Model:\s*([^\n]+)",
        "Model hash": r"Model hash:\s*([^\n]+)"
    }
    
    workflow_parts = []
    for name, pattern in param_patterns.items():
        if match := re.search(pattern, text, re.IGNORECASE):
            metadata["parameters"][name] = match.group(1).strip()
            workflow_parts.append(f"{name}: {match.group(1).strip()}")
    
    if workflow_parts:
        metadata["workflow"] = ", ".join(workflow_parts)

def check_nsfw(text):
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ NSFW —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–µ (—É–ª—É—á—à–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è)"""
    if not text:
        return False

    nsfw_keywords = {
        "cum", "bukkake", "nipple", "areola", "pussy", "anus", "asshole",
        "deepthroat", "fisting", "blowjob", "gangbang", "tentacle", "bondage", 
        "dildo", "vibrator", "orgasm", "penetration", "squirting", "cumshot",
        "creampie", "felching", "rimjob", "rimming", "anal", "dp", "gaping",
        "cock", "dick", "penis", "erection", "testicles", "scrotum", "glans",
        "clitoris", "clit", "labia", "vulva", "uncensored", "nude", "nudity",
        "masturbation", "fingering", "futa", "futanari", "shemale", "transgirl",
        "cumdump", "slut", "whore", "escort", "prostitute"
    }

    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞: —É–¥–∞–ª—è–µ–º –∑–Ω–∞–∫–∏ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏ –∏ –ø—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É
    normalized_text = re.sub(r'[^\w\s-]', '', text.lower())
    
    # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —Å–ª–æ–≤–∞, –≤–∫–ª—é—á–∞—è —Å–æ—Å—Ç–∞–≤–Ω—ã–µ (—á–µ—Ä–µ–∑ –¥–µ—Ñ–∏—Å/–ø–æ–¥—á—ë—Ä–∫–∏–≤–∞–Ω–∏–µ)
    words = set()
    for word in re.findall(r'[\w-]+', normalized_text):
        words.update(word.split('_'))  # –†–∞–∑–±–∏–≤–∞–µ–º snake_case
        words.update(word.split('-'))  # –†–∞–∑–±–∏–≤–∞–µ–º kebab-case
    
    return any(keyword in words for keyword in nsfw_keywords)


class UnicodeEncoder(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, str):
            return obj
        return super().default(obj)

def bytes_to_str(obj):
    if isinstance(obj, dict):
        return {k: bytes_to_str(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [bytes_to_str(i) for i in obj]
    elif isinstance(obj, bytes):
        try:
            return obj.decode('utf-8', errors='ignore')
        except:
            return str(obj)
    else:
        return obj   


# –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É settings.json (—Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–∞—è –ø–∞–ø–∫–∞)
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
ALAZUKA_JSON_PATH = os.path.join(BASE_DIR, "settings.json")

# POST - —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
@PromptServer.instance.routes.post("/alazuka/file/settings/post")
async def save_settings(request):
    data = await request.json()
    if not isinstance(data, dict):
        return web.json_response({"status": "error", "message": "Invalid data format, expected JSON object"}, status=400)

    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–µ–∫—É—â–∏–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
    if os.path.exists(ALAZUKA_JSON_PATH):
        with open(ALAZUKA_JSON_PATH, "r", encoding="utf-8") as f:
            settings = json.load(f)
    else:
        settings = {}

    # –û–±–Ω–æ–≤–ª—è–µ–º –∫–ª—é—á–∏ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞
    settings.update(data)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—Ä–∞—Ç–Ω–æ
    with open(ALAZUKA_JSON_PATH, "w", encoding="utf-8") as f:
        json.dump(settings, f, indent=4)

    return web.json_response({"status": "success", "message": "Settings updated", "updated_keys": list(data.keys())})

# GET - –ø–æ–ª—É—á–∏—Ç—å –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
@PromptServer.instance.routes.get("/alazuka/file/settings/get")
async def get_settings(request):
    if os.path.exists(ALAZUKA_JSON_PATH):
        with open(ALAZUKA_JSON_PATH, "r", encoding="utf-8") as f:
            settings = json.load(f)
    else:
        settings = {}
    return web.json_response(settings)

@PromptServer.instance.routes.get("/alazuka/file/{type}/{filename}")
async def serve_file(request):
    type = request.match_info["type"]
    filename = request.match_info["filename"]

    file_path = get_full_path(type, filename)
    if not file_path or not os.path.isfile(file_path):
        return web.Response(status=404)

    ext = os.path.splitext(filename)[1].lower()
    if ext == ".json":
        with open(file_path, "r", encoding="utf-8") as f:
            return web.Response(text=f.read(), content_type="application/json")

    return web.FileResponse(file_path)

@PromptServer.instance.routes.get("/alazuka/files/{type}")
async def get_grouped_files(request):
    type = request.match_info["type"]
    folders = folder_paths.get_folder_paths(type)
    grouped = {}

    # –°–Ω–∞—á–∞–ª–∞ –Ω–∞—Ö–æ–¥–∏–º –≤—Å–µ –º–æ–¥–µ–ª–∏ (safetensors, pt, ckpt –∏ —Ç.–¥.)
    model_exts = {"safetensors", "pt", "ckpt", "bin", "pth"}  # –º–æ–∂–Ω–æ —Ä–∞—Å—à–∏—Ä–∏—Ç—å
    image_exts = {"jpeg", "jpg", "png"}  # –º–æ–∂–Ω–æ —Ä–∞—Å—à–∏—Ä–∏—Ç—å

    for folder in folders:
        if not os.path.isdir(folder):
            continue

        # –°–∫–∞–Ω–∏—Ä—É–µ–º –ø–∞–ø–∫—É –∏ –≥—Ä—É–ø–ø–∏—Ä—É–µ–º —Ñ–∞–π–ª—ã
        for fname in os.listdir(folder):
            full_path = os.path.join(folder, fname)
            if not os.path.isfile(full_path):
                continue

            # –†–∞–∑–¥–µ–ª—è–µ–º –∏–º—è —Ñ–∞–π–ª–∞ –∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ
            if "." not in fname:
                continue
            base_part, ext = fname.rsplit(".", 1)
            ext = ext.lower()

            # –ï—Å–ª–∏ —ç—Ç–æ –º–æ–¥–µ–ª—å ‚Äî —Å–æ–∑–¥–∞—ë–º –∑–∞–ø–∏—Å—å
            if ext in model_exts:
                if base_part not in grouped:
                    grouped[base_part] = {
                        "model": f"{type}/{fname}",
                        "image": {},
                        "json": None
                    }

        # –¢–µ–ø–µ—Ä—å –∏—â–µ–º —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã (–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ JSON)
        for base_part in list(grouped.keys()):
            # –ò—â–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (–Ω–∞—á–∏–Ω–∞—é—Ç—Å—è –∫–∞–∫ base_part, –∑–∞–∫–∞–Ω—á–∏–≤–∞—é—Ç—Å—è –Ω–∞ image_exts)
            for fname in os.listdir(folder):
                if not fname.startswith(base_part + "."):
                    continue

                full_path = os.path.join(folder, fname)
                if not os.path.isfile(full_path):
                    continue

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ
                _, ext = fname.rsplit(".", 1)
                ext = ext.lower()

                if ext in image_exts:
                    meta_info = extract_metadata_from_media(full_path)
                    grouped[base_part]["image"][ext] = {
                        "path": f"{type}/{fname}",
                        "is_NSFW": meta_info.get("is_NSFW", False),
                        "prompt": meta_info.get("prompt", ""),
                        "negative_prompt": meta_info.get("negative_prompt", ""),
                        "workflow": meta_info.get("workflow", ""),
                        "meta_data": meta_info.get("exif", {}) or meta_info.get("text", {})
                    }

                elif ext == "json":
                    grouped[base_part]["json"] = f"{type}/{fname}"

    return web.json_response(grouped)